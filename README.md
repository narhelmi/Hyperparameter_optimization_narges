Hyperparameter Optimization
For this exercise, we will have a look at Hyperparameter Optimization -- instead of just choosing the best type of machine learning model, we also want to choose the best hyperparameter setting for a task. The end result (i.e. the predictive performance) is again not important; how you get there is.

Your deliverable will be a report, written in a style that it would be suitable for inclusion in an academic paper as the "Experimental Setup" section or similar. If unsure, check an academic paper of your choice, for example this one. The level of detail should be higher than in a typical academic paper though. Your report should be at most five pages, including references and figures but excluding appendices. It should have the following structure:

Introduction: What problem are you solving, how are you going to solve it.
Dataset Description: Describe the data you're using, e.g. how many features and observations, what are you predicting, any missing values, etc.
Experimental Setup: What specifically are you doing to solve the problem, i.e.\ what programming languages and libraries, how are you processing the data, what machine learning algorithms are you considering and what hyperparameters and value ranges, what measures you are using to evaluate them, what hyperparameter optimization method you chose, etc.
Results: Description of what you observed, including plots. Compare performance before and after tuning, and show the best configuration.
Code: Add the code you've used as a separate file.
Your report must contain enough detail to reproduce what you did without the code. If in doubt, include more detail.

There is no required format for the report. You could, for example, use an iPython notebook.

Data and Setup
We will have a look at the Wine Quality dataset. Choose the one that corresponds to your preference in wine. You may also use a dataset of your choice, for example one that's relevant to your research.

Choose a small number of different machine learning algorithms and hyperparameters, along with value ranges, for each. You can use implementations of AutoML systems (e.g. auto-sklearn), scientific papers, or the documentation of the library you are using to determine the hyperparameters to tune and the value ranges. Note that there is not only a single way to do this, but define a reasonable space (e.g. don't include whether to turn on debug output, or random forests with 1,000,000 trees, or tune the loss function). Your hyperparameter search space should be so large that you cannot simply run a grid search.

Determine the best machine learning algorithm and hyperparameter setting for your dataset. Make sure to optimize both the type of machine learning algorithm and the hyperparameters at the same time (do not first choose the best ML algorithm and then optimize its hyperparameters). Choose a suitable hyperparameter optimizer; you could also use several and e.g. compare the results achieved by random search and Bayesian optimization. Make sure that the way you evaluate model performance avoids bias and overfitting. You could use statistical tests to make this determination.

Submission
Add your report and code to this repository. Bonus points if you can set up a Github action to automatically run the code and generate the report!
